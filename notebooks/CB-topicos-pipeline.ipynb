{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detección de tópicos en ciencia básica: topicos con modelo entrenado\n",
    "\n",
    "1. Filtrar documentos\n",
    "2. Aplicar tf-idf\n",
    "3. Guardar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **To do** \n",
    "- detectar en inngles\n",
    "- Documentos repetidos entre repositorios y convocatorias\n",
    "- mismo proyecto \n",
    "-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielbustillos/opt/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os import listdir\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import heapq\n",
    "import seaborn as sns\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "from sklearn.externals import joblib \n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_texto_eval = pd.read_csv(\"./data_training.csv\").reset_index(drop=True,inplace=True)\n",
    "df_texto_eval = df_texto_eval.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Clean and Steamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str_series(s):\n",
    "\n",
    "    \"\"\"\n",
    "    Convierte caracteres de utf8 a ascii y elimina errores\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    s: string\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    s: string\n",
    "    \"\"\"\n",
    "\n",
    "    s = s.str.normalize('NFKD').str.encode(\n",
    "        'ascii', errors='ignore').str.decode('utf-8') \\\n",
    "        .str.capitalize().str.strip().str.replace('[^\\w\\s]', '')\n",
    "    return s\n",
    "\n",
    "def text_cleaner(df, columns_to_clean, columns_not_na):\n",
    "\n",
    "    \"\"\"\n",
    "    Elimina filas de un df en caso de ser vacías y aplica la función\n",
    "    clean_str_series\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: dataframe a limpiar\n",
    "    columns_to_clean: columnas a aplicar clean_str_series\n",
    "    columns_not_na: columnas a deshechar en caso de que sean NA\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df: dataframe con columnas limpias\n",
    "    \"\"\"\n",
    "\n",
    "    # Quitar registros no validos\n",
    "    df = df.dropna(subset=columns_not_na, axis=0)\n",
    "\n",
    "    # Formato texto\n",
    "    for d in columns_to_clean:\n",
    "        if df[d].dtype == object:\n",
    "            df[d] = clean_str_series(df[d])\n",
    "\n",
    "    return df\n",
    "\n",
    "def stemSentence(sentence, min_len=4):\n",
    "\n",
    "    \"\"\"\n",
    "    Aplica steamming a un string\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    sentence: string a aplicar steamming\n",
    "    min_len: mínimo de caracteres en palabras\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    stem_sentence: string con steamming\n",
    "    \"\"\"\n",
    "\n",
    "    token_words = word_tokenize(sentence)\n",
    "    stem_sentence = []\n",
    "\n",
    "    for word in token_words:\n",
    "        if len(word) > min_len:\n",
    "            stem_sentence.append(stemmer.stem(\n",
    "                WordNetLemmatizer().lemmatize(word, pos='v')))\n",
    "            stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TF-IDF\n",
    " regresa matriz documentos raices"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_texto_eval = pd.read_csv(\"./data/data_training.csv\").reset_index(drop=True)\n",
    "df_texto_eval = df_texto_eval.reset_index(drop=True)\n",
    "df_texto_eval = df_texto_eval.drop_duplicates(subset=[\"ID_PROYECTO\",\"NUMERO_CONVOCATORIA\",\"ANIO\"], keep=\"last\")\n",
    "df_texto_eval = df_texto_eval.sample(400)\n",
    "texto = df_texto_eval[\"DESCRIPCION_PROYECTO\"]\n",
    "len(texto)\n",
    "\n",
    "n_features = 512 #number of max words\n",
    "n_top_words = 30 #words per topic\n",
    "doc_similarity_thr = 0.15\n",
    "max_df = .15\n",
    "min_df = 5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text_data = df_texto_eval[\"DESCRIPCION_PROYECTO\"]\n",
    "text_data = text_data.apply(stemSentence)\n",
    "\n",
    "df_texto_eval[\"DESCRIPCION_PROYECTO\"] = text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_train(texto, max_df, min_df, n_features):\n",
    "\n",
    "    \"\"\"\n",
    "    Genera el vocabulario y la matriz de pesos usando tf-idf\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    texto: string a aplicar tfidf\n",
    "\n",
    "    max_df : float in range [0.0, 1.0] or int (default=1.0)\n",
    "        When building the vocabulary ignore terms that have a document\n",
    "        frequency strictly higher than the given threshold (corpus-specific\n",
    "        stop words).\n",
    "        If float, the parameter represents a proportion of documents,\n",
    "        integer absolute counts. This parameter is ignored if vocabulary\n",
    "        is not None.\n",
    "\n",
    "    min_df : float in range [0.0, 1.0] or int (default=1)\n",
    "        When building the vocabulary ignore terms that have a document\n",
    "        frequency strictly lower than the given threshold. This value is\n",
    "        also called cut-off in the literature.\n",
    "        If float, the parameter represents a proportion of documents,\n",
    "        integer absolute counts. This parameter is ignored if vocabulary\n",
    "        is not None.\n",
    "\n",
    "    n_features : int or None (default=None)\n",
    "        If not None, build a vocabulary that only consider the top\n",
    "        max_features ordered by term frequency across the corpus.\n",
    "        This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tfidf_vectorizer:  TfidfVectorizer fiteado\n",
    "\n",
    "    tfidf : sparse matrix, [n_samples, n_features]\n",
    "        Tf-idf-weighted document-term matrix.\n",
    "\n",
    "    tfidf_vectorizer: feature names de tfidf_vectorizer\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"...tfidf_train\")\n",
    "    \n",
    "    #definimos las stop words\n",
    "    with open(\"./pipeline/stop_words_spanish.txt\", 'r') as f:\n",
    "        stop_words_spanish = f.readlines()[0].split(\" \")\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df,\n",
    "                                       max_features=n_features,stop_words=stop_words_spanish)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(texto)\n",
    "    \n",
    "    \n",
    "    nombre_archivo_tf_idf = str(\"_max_df\" + str(max_df) + \"_min_df\" +  str(min_df) + \"_n_features\" + str(n_features))\n",
    "    #joblib.dump(tfidf_vectorizer, str('./trained_models/tfidf_vectorizer' + nombre_archivo_tf_idf + '.pkl'))\n",
    "    #joblib.dump(tfidf, str('./trained_models/tfidf' + nombre_archivo_tf_idf + '.pkl'))\n",
    "    \n",
    "    joblib.dump(tfidf_vectorizer, str('./trained_models/tfidf_vectorizer.pkl'))\n",
    "    joblib.dump(tfidf, str('./trained_models/tfidf.pkl'))\n",
    "    \n",
    "\n",
    "    return tfidf_vectorizer, tfidf, tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "def tfidf_test(texto, tfidf_vectorizer):\n",
    "\n",
    "    \"\"\"\n",
    "    Transforma nuevos textos usando tfidf_vectorizer\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    texto: list of strings a aplicar transformación usando tfidf_vectorizer\n",
    "\n",
    "    tfidf_vectorizer : model_tfidf\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tfidf_test: matriz de TFIDF para nuevos textos.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"...tfidf_test\")\n",
    "\n",
    "    tfidf_test = tfidf_vectorizer.transform(texto)\n",
    "    return tfidf_test\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tfidf_vectorizer, tfidf, feature_names = tfidf_train(texto, max_df, min_df,n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tópicos por NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the NMF model (generalized Kullback-Leibler divergence)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_components = 50\n",
    "n_components # borrar con nuevos docs\n",
    "max_iter = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train_nmf(tfidf, n_components, beta_loss='kullback-leibler',\n",
    "                  solver='mu', max_iter=100, alpha=.1, l1_ratio=.5):\n",
    "        \"\"\"\n",
    "        Genera tópicos usando matríz de tfidf\n",
    "\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        tfidf: string\n",
    "            matriz de pesos generado en tfidf\n",
    "\n",
    "        n_components : int or None\n",
    "            Number of components, if n_components is not set all features\n",
    "            are kept.\n",
    "\n",
    "        beta_loss : float or string, default ‘frobenius’\n",
    "            String must be in {‘frobenius’, ‘kullback-leibler’,\n",
    "             ‘itakura-saito’}. Beta divergence to be minimized, measuring\n",
    "             the distance between X and the dot product WH. Note that\n",
    "             values different from ‘frobenius’ (or 2) and\n",
    "             ‘kullback-leibler’ (or 1) lead to significantly slower fits.\n",
    "             Note that for beta_loss <= 0 (or ‘itakura-saito’), the input\n",
    "             matrix X cannot contain zeros. Used only in ‘mu’ solver.\n",
    "\n",
    "\n",
    "        solver : ‘cd’ | ‘mu’\n",
    "            Numerical solver to use: ‘cd’ is a Coordinate Descent solver.\n",
    "            ‘mu’ is a Multiplicative Update solver.\n",
    "\n",
    "        max_iter : integer, default: 100\n",
    "            Maximum number of iterations before timing out.\n",
    "\n",
    "        max_iter : integer, default: 200\n",
    "            Maximum number of iterations before timing out.\n",
    "\n",
    "        1_ratio : double, default: 0.\n",
    "            The regularization mixing parameter, with 0 <= l1_ratio <= 1\n",
    "            For l1_ratio = 0 the penalty is an elementwise L2 penalty\n",
    "            (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1\n",
    "            penalty. For 0 < l1_ratio < 1, the penalty is a combination\n",
    "            of L1 and L2.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        topic_model: matrix or sparse array\n",
    "            matriz de tópicos generados por NMF.\n",
    "        \"\"\"\n",
    "        print(\"...train_nmf\")\n",
    "\n",
    "        topic_model = NMF(n_components, random_state=123,\n",
    "                          beta_loss=beta_loss,\n",
    "                          solver=solver, max_iter=max_iter,\n",
    "                          alpha=alpha, l1_ratio=l1_ratio)\n",
    "\n",
    "        topic_model.fit(tfidf)\n",
    "        \n",
    "        joblib.dump(topic_model, str('./trained_models/topic_model.pkl'))\n",
    "\n",
    "        return topic_model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "topic_model = train_nmf(tfidf, n_components, beta_loss='kullback-leibler',\n",
    "              solver='mu', max_iter=200, alpha=.1,l1_ratio=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardar vocabulario y pesos los topicos generados por NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulario_nmf(topic_model, feature_names):\n",
    "    \"\"\"\n",
    "    Genera un diccionario con el index,palabra y peso de cada topico y lo\n",
    "    pasa a un DF\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    topic_model: matrix\n",
    "        matriz generado usando NMF matriz de pesos generado en tfidf\n",
    "\n",
    "    feature_names: list\n",
    "        nombre de palabras de diccionario.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df_topicos: matrix or sparse array\n",
    "        matriz de tópicos generados por NMF.\n",
    "    \"\"\"\n",
    "    print(\"...vocabulario_nmf\")\n",
    "\n",
    "    topic_data = []\n",
    "    for topic_idx, topic in enumerate(topic_model.components_):\n",
    "        index = [i for i in range(len(topic))]\n",
    "        words = [feature_names[i] for i in index]\n",
    "        value = [topic[i] for i in index]\n",
    "\n",
    "        topic_data.append({\"index\": index, \"words\": words, \"value\": value})\n",
    "\n",
    "    filter_id = \"topic-\"\n",
    "    df_topicos = pd.DataFrame([t['value'] for t in topic_data])\n",
    "    df_topicos.index = [filter_id + str(t).zfill(3)\n",
    "                        for t in df_topicos.index]\n",
    "\n",
    "    df_topicos.columns = topic_data[0]['words']\n",
    "    joblib.dump(df_topicos, './trained_models/df_topicos.pkl')\n",
    "    return df_topicos, topic_data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_topicos, topic_data = vocabulario_nmf(topic_model,feature_names)\n",
    "df_topicos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 filtro de valores de NMF por threshold\n",
    "\n",
    "Nos quedamos con las topk palabras de la matriz de componentes de NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtro_vactores_nmf(thresh_percentile, topic_data):\n",
    "    \"\"\"\n",
    "    filtra los valores de peso de cada tópico de nmf segun el\n",
    "    percentil del topico para cada topico\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    thresh_percentile: float in [0,100]\n",
    "        percentil para filtrar\n",
    "\n",
    "    topic_data: NMF model\n",
    "        Matriz NMF\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    topic_data: NMF model\n",
    "        Matriz NMF\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"...filtro_vactores_nmf\")\n",
    "\n",
    "    for topic in range(len(topic_data)):\n",
    "\n",
    "        # valor de el filtro a partir de percentile\n",
    "        thresh_filter = np.percentile(np.array(\n",
    "            topic_data[topic][\"value\"]), thresh_percentile)\n",
    "\n",
    "        values_filtered = [row if row > thresh_filter else 0\n",
    "                           for row in topic_data[topic][\"value\"]]\n",
    "        topic_data[topic][\"value\"] = values_filtered\n",
    "\n",
    "    joblib.dump(topic_data, './trained_models/topic_model_threshold.pkl')\n",
    "\n",
    "    return topic_data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "thresh_percentile = 95\n",
    "\n",
    "topic_data = filtro_vactores_nmf(thresh_percentile,topic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. topicos por texto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asignamos un vectore de topicos para cada texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topico_a_texto(df_topicos, tfidf, topic_data, status=\"test\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Añade el vector de topicos a partir del producto punto entre la matriz\n",
    "    de topicos de  NMF y la matrz de pesos de TFidf de cada documento.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_topicos: matrix or sparse array\n",
    "        matriz de tópicos generados por NMF.\n",
    "\n",
    "    tfidf : sparse matrix, [n_samples, n_features]\n",
    "        Tf-idf-weighted document-term matrix.\n",
    "\n",
    "    topic_data: NMF model\n",
    "        Matriz NMF\n",
    "    Returns:\n",
    "    --------\n",
    "    dataframe_values: df\n",
    "        dataframe con topico por columna para cada texto\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"...topico_a_texto\")\n",
    "\n",
    "    lista_topicos = df_topicos.index.tolist()\n",
    "    dataframe_values = pd.DataFrame(columns=lista_topicos,\n",
    "                                    index=[row for row in range(\n",
    "                                        tfidf[:].shape[0])])\n",
    "\n",
    "    topics_results = []\n",
    "\n",
    "    # computa el producto punto\n",
    "    for i_doc in range(tfidf[:].shape[0]):\n",
    "        valor_topico = [\n",
    "            np.dot(\n",
    "                tfidf[i_doc].todense().tolist()[0],\n",
    "                topic_data[topic_id][\"value\"])\n",
    "            for topic_id in range(len(topic_data))]  # pesos de topicos\n",
    "\n",
    "        dataframe_values.iloc[i_doc, :] = valor_topico\n",
    "        topics_results.append(str(valor_topico))\n",
    "\n",
    "        # guardamos el vector generado para elastic solo en enntrenamiento\n",
    "    if status == \"train\":\n",
    "        df_texto_eval[\"topic_vector\"] = topics_results\n",
    "        # TF_idf_vector\n",
    "        df_texto_eval[\"tfidf_vector\"] = [tfidf.toarray()[row] for row\n",
    "                                         in range(len(df_texto_eval))]\n",
    "\n",
    "        joblib.dump(df_texto_eval, './trained_models/df_texto_eval.pkl')\n",
    "\n",
    "    return dataframe_values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataframe_values = topico_a_texto(df_topicos,tfidf,topic_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataframe_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.  Vector por evaluador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converetimos a numeriico cols de peso y pegamos la columna ID_PROYECTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topico_a_texto(df_topicos, tfidf, topic_data, status=\"test\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Añade el vector de topicos a partir del producto punto entre la matriz\n",
    "    de topicos de  NMF y la matrz de pesos de TFidf de cada documento.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_topicos: matrix or sparse array\n",
    "        matriz de tópicos generados por NMF.\n",
    "\n",
    "    tfidf : sparse matrix, [n_samples, n_features]\n",
    "        Tf-idf-weighted document-term matrix.\n",
    "\n",
    "    topic_data: NMF model\n",
    "        Matriz NMF\n",
    "    Returns:\n",
    "    --------\n",
    "    dataframe_values: df\n",
    "        dataframe con topico por columna para cada texto\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"...topico_a_texto\")\n",
    "\n",
    "    lista_topicos = df_topicos.index.tolist()\n",
    "    dataframe_values = pd.DataFrame(columns=lista_topicos,\n",
    "                                    index=[row for row in range(\n",
    "                                        tfidf[:].shape[0])])\n",
    "\n",
    "    topics_results = []\n",
    "\n",
    "    # computa el producto punto\n",
    "    for i_doc in range(tfidf[:].shape[0]):\n",
    "        valor_topico = [\n",
    "            np.dot(\n",
    "                tfidf[i_doc].todense().tolist()[0],\n",
    "                topic_data[topic_id][\"value\"])\n",
    "            for topic_id in range(len(topic_data))]  # pesos de topicos\n",
    "\n",
    "        dataframe_values.iloc[i_doc, :] = valor_topico\n",
    "        topics_results.append(str(valor_topico))\n",
    "\n",
    "        # guardamos el vector generado para elastic solo en enntrenamiento\n",
    "    if status == \"train\":\n",
    "        df_texto_eval[\"topic_vector\"] = topics_results\n",
    "        # TF_idf_vector\n",
    "        df_texto_eval[\"tfidf_vector\"] = [tfidf.toarray()[row] for row\n",
    "                                         in range(len(df_texto_eval))]\n",
    "\n",
    "        joblib.dump(df_texto_eval, './trained_models/.pkl')\n",
    "\n",
    "    return dataframe_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def texto_a_evaluador(dataframe_values, status=\"test\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Asigna el vector de topicos de cada texto revisado por un evaluador y\n",
    "    devuelve el vector promedio de todos los textos que ha evaluado.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_topicos: matrix or sparse array\n",
    "        matriz de tópicos generados por NMF.\n",
    "\n",
    "    tfidf : sparse matrix, [n_samples, n_features]\n",
    "        Tf-idf-weighted document-term matrix.\n",
    "\n",
    "    topic_data: NMF model\n",
    "        Matriz NMF\n",
    "    Returns:\n",
    "    --------\n",
    "    dataframe_values: df\n",
    "        dataframe con topico por columna para cada texto\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Asigna el vector de topicos de cada texto revisado por un evaluador y\n",
    "    devuelve el vector promedio\n",
    "    de todos los textos que ha evaluado.\n",
    "    \"\"\"\n",
    "    print(\"...texto_a_evaluador\")\n",
    "\n",
    "    index_dataframe_values = dataframe_values.columns.tolist()[:]\n",
    "\n",
    "    dataframe_values[index_dataframe_values] = dataframe_values[\n",
    "        index_dataframe_values].apply(\n",
    "        pd.to_numeric, errors='coerce').reset_index(drop=True)\n",
    "\n",
    "# informacion de proyecto y evaluador\n",
    "\n",
    "    dataframe_values[\"ID_PROYECTO\"] = df_texto_eval[\"ID_PROYECTO\"\n",
    "                                                    ].reset_index(\n",
    "        drop=True)\n",
    "    # poner campos en primera posicion\n",
    "    dataframe_values = dataframe_values.set_index(\n",
    "        [\"ID_PROYECTO\"]).reset_index(drop=False)\n",
    "\n",
    "    if status == \"train\":\n",
    "        # merge con evaluadores\n",
    "        df_info_eval = pd.read_csv(\n",
    "            \"./data/data_training.csv\").reset_index(drop=True)\n",
    "        df_info_eval = df_info_eval[[\"ID_PROYECTO\", \"USUARIO\",\n",
    "                                    \"CVU\", \"CVE_RCEA\"]]\n",
    "\n",
    "        dataframe_values = df_info_eval.merge(dataframe_values,\n",
    "                                              on=\"ID_PROYECTO\",\n",
    "                                              how=\"inner\")\n",
    "\n",
    "    # groupby de los vectores por evaluador\n",
    "        topics_evaluador = dataframe_values.groupby(\n",
    "            [\"CVE_RCEA\", \"USUARIO\"])[index_dataframe_values[3:]].mean()\n",
    "\n",
    "        joblib.dump(topics_evaluador,\n",
    "                    './trained_models/topics_evaluador.pkl')\n",
    "        return topics_evaluador\n",
    "    else:\n",
    "        joblib.dump(dataframe_values,\n",
    "                    './trained_models/topicos_port_texto_test.pkl')\n",
    "        return dataframe_values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "topics_evaluador = texto_a_evaluador(dataframe_values,status=\"train\")\n",
    "topics_evaluador.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_topic_train(texto, max_df, min_df,n_features,   #tfidf\n",
    "                    n_components, beta_loss,solver,l1_ratio, # NMF\n",
    "                  thresh_percentile):   # filtro de vectores por topico \n",
    "    \"\"\"\n",
    "    Pipeline de entreneamiento: \n",
    "        con el conjunto de entrenamiento:\n",
    "        1. TFIDF\n",
    "        2. NMF\n",
    "        3. ajusta treshhold de vectores de NMF\n",
    "    \n",
    "    Output:\n",
    "        1. tfidf_vectorizer\n",
    "        2. topic_model\n",
    "        \n",
    "    \"\"\"\n",
    "    # tfidf\n",
    "    tfidf_vectorizer, tfidf, feature_names = tfidf_train(texto, max_df, min_df,n_features)\n",
    "    \n",
    "    #NMF\n",
    "    topic_model = train_nmf(tfidf, n_components, beta_loss='kullback-leibler',\n",
    "              solver='mu', max_iter=200, alpha=.1,l1_ratio=.5)\n",
    "    \n",
    "    #guardar vocabulario (no necesario)\n",
    "    df_topicos, topic_data = vocabulario_nmf(topic_model,feature_names)\n",
    "    \n",
    "    # filtro de vector por umbral\n",
    "    topic_data = filtro_vactores_nmf(thresh_percentile,topic_data)\n",
    "\n",
    "    dataframe_values = topico_a_texto(df_topicos,tfidf,topic_data,status=\"train\")\n",
    "    \n",
    "    topics_evaluador = texto_a_evaluador(dataframe_values,status=\"train\")\n",
    "    \n",
    "    return tfidf_vectorizer,topic_model,topics_evaluador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_cleaning_steamming(data_topics,columns_not_na,columns_interes):\n",
    "    \n",
    "    \"\"\"\n",
    "    limpieza y steammingtest dataset\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    data_topics_clean = text_cleaner(data_topics, columns_to_clean=columns_interes,\n",
    "                                     columns_not_na=columns_not_na)\n",
    "    \n",
    "    \n",
    "    # steamming\n",
    "    text_data = data_topics_clean[\"DESCRIPCION_PROYECTO\"]\n",
    "    text_data = text_data.apply(stemSentence)\n",
    "\n",
    "    data_topics_clean[\"DESCRIPCION_PROYECTO\"] = text_data\n",
    "    data_topics_clean.reset_index(drop=True,inplace=True)\n",
    "    return data_topics_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_topic_test(data_topics, tfidf_vectorizer, topic_data, to_clean=1):\n",
    "    # filtro de vectores por topico\n",
    "    \"\"\"\n",
    "    Pipeline de nuevos datos:\n",
    "        con el conjunto de entrenamiento:\n",
    "        1. TFIDF usando el vectorizador ya entrenado\n",
    "        2. obtener valores de tfidf.NMF usando la matriz entrenada\n",
    "        3. devuelve cada texto con su peso\n",
    "    \"\"\"\n",
    "    # pipeline limpieza\n",
    "\n",
    "    if to_clean == 1:\n",
    "        columns_not_na = [\"PALABRAS_CLAVE1\", \"PALABRAS_CLAVE2\",\n",
    "                          \"PALABRAS_CLAVE3\", \"DESCRIPCION_PROYECTO\"]\n",
    "        columns_interes = [\"DESCRIPCION_PROYECTO\"]\n",
    "\n",
    "        data_topics = pipeline_cleaning_steamming(\n",
    "            data_topics, columns_not_na=columns_not_na,\n",
    "            columns_interes=columns_interes)\n",
    "\n",
    "    texto = data_topics[\"DESCRIPCION_PROYECTO\"]\n",
    "\n",
    "    tfidf_test_1 = tfidf_test(texto, tfidf_vectorizer)\n",
    "    # NMF\n",
    "\n",
    "    # guardar vocabulario (no necesario)\n",
    "    # df_topicos, topic_data = vocabulario_nmf(topic_model,feature_names)\n",
    "\n",
    "    # asigna el vector de NMF a cada texto\n",
    "    dataframe_values = topico_a_texto(df_topicos,tfidf,topic_data,status=\"test\")\n",
    "    # promedia el valor de los vectores de cada texto evaluado por un evaluador\n",
    "    dataframe_values = texto_a_evaluador(dataframe_values)\n",
    "\n",
    "    return dataframe_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_entrenaiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texto_eval = pd.read_csv(\"./data/data_training.csv\")\n",
    "df_texto_eval = df_texto_eval.reset_index(drop=True)\n",
    "df_texto_eval = df_texto_eval.drop_duplicates(subset=[\"ID_PROYECTO\",\"NUMERO_CONVOCATORIA\",\"ANIO\"], keep=\"last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFidf params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = df_texto_eval[\"DESCRIPCION_PROYECTO\"]\n",
    "n_features = 512 #number of max words\n",
    "#n_top_words = 30 #words per topic\n",
    "#doc_similarity_thr = 0.15\n",
    "max_df = .15\n",
    "min_df = 5\n",
    "#prueba de distribucion de pesos, ponemos lo defaul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 50\n",
    "n_components # borrar con nuevos docs\n",
    "max_iter = 30\n",
    "beta_loss='kullback-leibler'\n",
    "solver='mu'\n",
    "alpha=.1\n",
    "l1_ratio=.5\n",
    "#### thresh\n",
    "thresh_percentile = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...tfidf_train\n",
      "...train_nmf\n",
      "...vocabulario_nmf\n",
      "...filtro_vactores_nmf\n",
      "...topico_a_texto\n",
      "...texto_a_evaluador\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer,topic_model, topics_evaluador = pipeline_topic_train(texto, max_df, min_df,n_features,   #tfidf\n",
    "                    n_components, beta_loss,solver,l1_ratio, # NMF\n",
    "                  thresh_percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cargamos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './trained_models/df_texto_eval.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ce5f2530ec90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtfidf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./trained_models/tfidf.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_topicos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./trained_models/df_topicos.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_texto_eval_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./trained_models/df_texto_eval.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtopics_evaluador_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./trained_models/topics_evaluador.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './trained_models/df_texto_eval.pkl'"
     ]
    }
   ],
   "source": [
    "topic_data_train = joblib.load('./trained_models/topic_model_threshold.pkl')\n",
    "tfidf_vectorizer_train = joblib.load('./trained_models/tfidf_vectorizer.pkl')\n",
    "tfidf_train = joblib.load('./trained_models/tfidf.pkl')\n",
    "df_topicos = joblib.load('./trained_models/df_topicos.pkl')\n",
    "df_texto_eval_train = joblib.load('./trained_models/df_texto_eval.pkl')\n",
    "topics_evaluador_train = joblib.load('./trained_models/topics_evaluador.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texto_eval = pd.read_csv(\"./data/data_training.csv\").reset_index(drop=True)\n",
    "df_texto_eval = df_texto_eval.reset_index(drop=True)\n",
    "df_texto_eval = df_texto_eval.drop_duplicates(\n",
    "    subset=[\"ID_PROYECTO\", \"NUMERO_CONVOCATORIA\", \"ANIO\"], keep=\"last\")\n",
    "\n",
    "target = df_texto_eval[[\"USUARIO\", \"CVU\", \"CVE_RCEA\"]]\n",
    "df_texto_eval.drop([\"USUARIO\", \"CVU\", \"CVE_RCEA\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texto_eval.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe_values = pipeline_topic_test(\n",
    "    df_texto_eval, tfidf_vectorizer_train, topic_data_train, to_clean=0)\n",
    "print(dataframe_values.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
