{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_article = \"Topological properties of networks are widely applied to study the link-prediction problem recently. Common Neighbors, for example, is a natural yet efficient framework. Many variants of Common Neighbors have been thus proposed to further boost the discriminative resolution of candidate links. In this paper, we reexamine the role of network topology in predicting missing links from the perspective of information theory, and present a practical approach based on the mutual information of network structures. It not only can improve the prediction accuracy substantially, but also experiences reasonable computing complexity.Go to:IntroductionLink prediction attempts to estimate the likelihood of the existence of links between nodes based on the available network information, such as the observed links and nodes' attributes [1], [2]. On the one hand, the link-prediction problem is a long-standing practical scientific issue. It can find broad applications in both identifying missing and spurious links and predicting the candidate links that are expected to appear with the evolution of networks [1], [3], [4]. In biological networks (such as protein-protein interaction networks [5] and metabolic networks [6]), for example, the discovery of interactions is usually costly. Therefore, accurate prediction is more reasonable compared with blindly checking all latent interaction links [3], [4]. In addition, the detection of inactive or anomalous connections in online social networks may improve the performance of link-based ranking algorithms [7]. Furthermore, in online social networks, very promising candidate links (non-connected node pairs) can be recommended to the relevant users as potential friendships [8], [9]. It can help them to find new friends and thus enhance their loyalties to the web sites. In ref. [9], the authors even proposed the potential theory to facilitate the missing link prediction of directed networks. The hypothesis can find broad applications in friendship recommendation of large-scale directed social networks, such as Twitter, Weibo and so on. On the other hand, theoretically, link prediction can provide a useful methodology for the modeling of networks [10]. The evolving mechanisms of networks have been widely studied. Many evolving models have been proposed to capture the evolving process of real-world networks [11]–[14]. However, it is very hard to quantify the degree to which the proposed evolving models govern real networks. Actually, each evolving model can be viewed as the corresponding predictor, we can thus apply evaluating metrics on prediction accuracy to measure the performance of different models.Therefore, link prediction has attracted much attention from various scientific communities. Within computer society, for example, scientists have employed Markov chains [15], [16] and machine learning techniques [17]–[21] to extract features of networks. These methods, however, depend on the attributes of nodes for particular networks such as social and textual features. Obviously, the attributes of nodes are generally hidden, and it is thus difficult for people to obtain them [2].Over the last 15 years, network science has been developed as a novel framework for understanding structures of many real-world networked systems. Recently, a wealth of algorithms based on structural information have been proposed [2], [4], [22]–[28]. Among various node-neighbor-based indices, Common Neighbors (CN) is undoubtedly the precursor with low computing complexity. It has also been revealed that CN achieves high prediction accuracy compared with other classical prediction indices [25]. CN, however, only emphasizes the number of common neighbors but ignores the difference in their contributions. In this case, several variants of CN to correct such a defect were put forwarded. Consider, for example, Adamic-Adar [24] and Resource Allocation [25], in which low-degree common neighbors are advocated by assigning more weight to them. In addition, based on the Bayesian theory, a Local Nave Bayes model [27] was presented to differentiate the roles of neighboring nodes. Furthermore, node centrality (including degree, closeness and betweenness) was applied to make neighbors more distinguishable. Besides such CN-based indices, the evolving patterns and organizing principles of networks can also provide useful insights for coping with the link-prediction problem. The well-known mechanism of preferential attachment [11], for instance, has been viewed as a prediction measure [25], [29]. For networks exhibiting hierarchical structure, Hierarchical Random Graph can be employed to predict missing links accordingly [4]. Recently, communities have been reinvented as groups of links rather than nodes [30]. Motivated by the shift in perspective of communities, Cannistraci et al. developed the local-community-paradigm to enhance the performance of classical prediction techniques [28].All the aforementioned methods aim to quantify the existence likelihood of candidate links. In information theory, the likelihood can be measured by the self-information. In this article, we thus try to give a more theoretical analysis of the link-prediction problem from the perspective of information theory. Then a general prediction approach based on mutual information is presented accordingly. Our framework outperforms other prediction methods greatly.Go to:ResultsA Mutual Information Approach to Link PredictionWe here introduce the definitions of the self-information and of the mutual information, respectively.Definition 1 Considering a random variable An external file that holds a picture, illustration, etc.Object name is pone.0107056.e001.jpg associated with outcome An external file that holds a picture, illustration, etc.Object name is pone.0107056.e002.jpg with probability An external file that holds a picture, illustration, etc.Object name is pone.0107056.e003.jpg, its self-information An external file that holds a picture, illustration, etc.Object name is pone.0107056.e004.jpg can be denoted as [31]equation image\t(1)where the base of the logarithm is specified as 2, thus the unit of self-information is bit. This is applicable for the following if not otherwise specified. The self-information indicates the uncertainty of the outcome An external file that holds a picture, illustration, etc.Object name is pone.0107056.e006.jpg. Obviously, the higher the self-information is, the less likely the outcome An external file that holds a picture, illustration, etc.Object name is pone.0107056.e007.jpg occurs.Definition 2 Consider two random variables An external file that holds a picture, illustration, etc.Object name is pone.0107056.e008.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0107056.e009.jpg with a joint probability mass function An external file that holds a picture, illustration, etc.Object name is pone.0107056.e010.jpg and marginal probability mass functions An external file that holds a picture, illustration, etc.Object name is pone.0107056.e011.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0107056.e012.jpg. The mutual information An external file that holds a picture, illustration, etc.Object name is pone.0107056.e013.jpg can be denoted as follows [32]:equation image\t(2)Hence, the mutual information An external file that holds a picture, illustration, etc.Object name is pone.0107056.e015.jpg can be obtained asequation image\t(3)The mutual information is the reduction in uncertainty due to another variable. Thus, it is a measure of the dependence between two variables. It is equal to zero if and only if two variables are independent.Now consider the link-prediction problem. Our idea is to use the local structural information to facilitate the prediction. To do that, we denote the set of node An external file that holds a picture, illustration, etc.Object name is pone.0107056.e017.jpg's neighboring nodes by An external file that holds a picture, illustration, etc.Object name is pone.0107056.e018.jpg. For node pair An external file that holds a picture, illustration, etc.Object name is pone.0107056.e019.jpg, the set of their common neighbors is denoted as An external file that holds a picture, illustration, etc.Object name is pone.0107056.e020.jpg.Given a disconnected node pair An external file that holds a picture, illustration, etc.Object name is pone.0107056.e021.jpg, if the set of their common neighbors An external file that holds a picture, illustration, etc.Object name is pone.0107056.e022.jpg is available, the likelihood score of node pair An external file that holds a picture, illustration, etc.Object name is pone.0107056.e023.jpg is defined asequation image\t(4)where An external file that holds a picture, illustration, etc.Object name is pone.0107056.e025.jpg is the conditional self-information of the existence of a link between node pair An external file that holds a picture, illustration, etc.Object name is pone.0107056.e026.jpg when their common neighbors are known. According to the property of the self-information, the smaller An external file that holds a picture, illustration, etc.Object name is pone.0107056.e027.jpg is, the higher the likelihood of existence of links is. Thus, we define the score as the negation of An external file that holds a picture, illustration, etc.Object name is pone.0107056.e028.jpg. According to the definition of mutual information, An external file that holds a picture, illustration, etc.Object name is pone.0107056.e029.jpg can thus be derived asequation image\t(5)where An external file that holds a picture, illustration, etc.Object name is pone.0107056.e031.jpg is the self-information of that node pair An external file that holds a picture, illustration, etc.Object name is pone.0107056.e032.jpg is connected. An external file that holds a picture, illustration, etc.Object name is pone.0107056.e033.jpg is the mutual information between the event that node pair An external file that holds a picture, illustration, etc.Object name is pone.0107056.e034.jpg has one link between them and the event that the node pair's common neighbors are known. Note that An external file that holds a picture, illustration, etc.Object name is pone.0107056.e035.jpg is calculated by the prior probability of that node An external file that holds a picture, illustration, etc.Object name is pone.0107056.e036.jpg and node An external file that holds a picture, illustration, etc.Object name is pone.0107056.e037.jpg are connected. In our method, without knowing the common neighbors of node pair An external file that holds a picture, illustration, etc.Object name is pone.0107056.e038.jpg, we could use An external file that holds a picture, illustration, etc.Object name is pone.0107056.e039.jpg to estimate the existence of a link between node pair An external file that holds a picture, illustration, etc.Object name is pone.0107056.e040.jpg. An external file that holds a picture, illustration, etc.Object name is pone.0107056.e041.jpg indicates the reduction in uncertainty of the connection between nodes An external file that holds a picture, illustration, etc.Object name is pone.0107056.e042.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0107056.e043.jpg due to the information given by their common neighbors. Since the mutual information plays a significant role in our method, this framework is called MI for short.If the elements of An external file that holds a picture, illustration, etc.Object name is pone.0107056.e044.jpg are assumed to be independent of each other, thenequation image\t(6)Here An external file that holds a picture, illustration, etc.Object name is pone.0107056.e046.jpg can be estimated by An external file that holds a picture, illustration, etc.Object name is pone.0107056.e047.jpg, which is defined as the average mutual information over all node pairs connected to node An external file that holds a picture, illustration, etc.Object name is pone.0107056.e048.jpgequation image\t(7)Now we try to calculate the above mutual information. According to its definition (3), An external file that holds a picture, illustration, etc.Object name is pone.0107056.e050.jpg can be denoted asequation image\t(8)where An external file that holds a picture, illustration, etc.Object name is pone.0107056.e052.jpg is the conditional self-information of that node pair An external file that holds a picture, illustration, etc.Object name is pone.0107056.e053.jpg is connected when node An external file that holds a picture, illustration, etc.Object name is pone.0107056.e054.jpg is one of their common neighbors, and An external file that holds a picture, illustration, etc.Object name is pone.0107056.e055.jpg denotes the self-information of that node pair An external file that holds a picture, illustration, etc.Object name is pone.0107056.e056.jpg has one link. The right-hand side of eq. (8) is composed of the (conditional) self-information. Based on the definition of (conditional) self-information, it can be calculated based on the (conditional) probability.The conditional probability An external file that holds a picture, illustration, etc.Object name is pone.0107056.e057.jpg can be estimated by the clustering coefficient of node An external file that holds a picture, illustration, etc.Object name is pone.0107056.e058.jpg, defined asequation image\t(9)where An external file that holds a picture, illustration, etc.Object name is pone.0107056.e060.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0107056.e061.jpg are the numbers of connected and of disconnected node pairs with node An external file that holds a picture, illustration, etc.Object name is pone.0107056.e062.jpg being a common neighbor, respectively. Once An external file that holds a picture, illustration, etc.Object name is pone.0107056.e063.jpg is available, An external file that holds a picture, illustration, etc.Object name is pone.0107056.e064.jpg can be calculated.In order to calculate the probability An external file that holds a picture, illustration, etc.Object name is pone.0107056.e065.jpg, we assume that no degree-degree correlation is considered. When nodes' degrees are known, the probability that node pair An external file that holds a picture, illustration, etc.Object name is pone.0107056.e066.jpg is disconnected is derived asequation image\t(10)where An external file that holds a picture, illustration, etc.Object name is pone.0107056.e068.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0107056.e069.jpg are the degrees of nodes An external file that holds a picture, illustration, etc.Object name is pone.0107056.e070.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0107056.e071.jpg, respectively. An external file that holds a picture, illustration, etc.Object name is pone.0107056.e072.jpg is the total number of links in the training set. Obviously this formula is symmetric, namelyequation image\t(11)Thus,equation image\t(12)and An external file that holds a picture, illustration, etc.Object name is pone.0107056.e075.jpg can be calculated accordingly.Collecting these results, we can obtainequation image\t(13)It is stipulated that An external file that holds a picture, illustration, etc.Object name is pone.0107056.e077.jpg if An external file that holds a picture, illustration, etc.Object name is pone.0107056.e078.jpg.Based on the above derivation, we haveequation image\t(14)where An external file that holds a picture, illustration, etc.Object name is pone.0107056.e080.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0107056.e081.jpg can be calculated by eqs. (13) and (12) respectively.To facilitate the understanding of MI, we illustrate it with an example as shown in fig. 1. First, consider node An external file that holds a picture, illustration, etc.Object name is pone.0107056.e082.jpg, for example, which is the common neighbor of nodes An external file that holds a picture, illustration, etc.Object name is pone.0107056.e083.jpg, An external file that holds a picture, illustration, etc.Object name is pone.0107056.e084.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0107056.e085.jpg. Using eq. (9), we can have An external file that holds a picture, illustration, etc.Object name is pone.0107056.e086.jpg. Based on eq. (12), we obtain An external file that holds a picture, illustration, etc.Object name is pone.0107056.e087.jpg, An external file that holds a picture, illustration, etc.Object name is pone.0107056.e088.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0107056.e089.jpg. Hence, we have An external file that holds a picture, illustration, etc.Object name is pone.0107056.e090.jpg. Now we compare node pairs An external file that holds a picture, illustration, etc.Object name is pone.0107056.e091.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0107056.e092.jpg with the common neighbor node An external file that holds a picture, illustration, etc.Object name is pone.0107056.e093.jpg. Then An external file that holds a picture, illustration, etc.Object name is pone.0107056.e094.jpg, An external file that holds a picture, illustration, etc.Object name is pone.0107056.e095.jpg, which can be calculated based on eq. (5). That is to say, node pair An external file that holds a picture, illustration, etc.Object name is pone.0107056.e096.jpg is more likely to be connected than node pair An external file that holds a picture, illustration, etc.Object name is pone.0107056.e097.jpg. The six prediction methods mentioned in section “Previous Prediction Methods”, however, cannot distinguish these two node pairs. In this sense, MI has higher discriminative resolution than them. Second, MI can distinguish node pairs even if they all have no common neighbors. For instance, An external file that holds a picture, illustration, etc.Object name is pone.0107056.e098.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0107056.e099.jpg. That is to say, node pair An external file that holds a picture, illustration, etc.Object name is pone.0107056.e100.jpg is more likely to be connected than node pair An external file that holds a picture, illustration, etc.Object name is pone.0107056.e101.jpg. This is undoubtedly beyond the distinguishing ability of previous methods. Thirdly, the mutual information of node An external file that holds a picture, illustration, etc.Object name is pone.0107056.e102.jpg can be calculated as An external file that holds a picture, illustration, etc.Object name is pone.0107056.e103.jpg. Thus An external file that holds a picture, illustration, etc.Object name is pone.0107056.e104.jpg. We note that An external file that holds a picture, illustration, etc.Object name is pone.0107056.e105.jpg, namely, node pair An external file that holds a picture, illustration, etc.Object name is pone.0107056.e106.jpg with two common neighbors has higher connection likelihood compared to node pair An external file that holds a picture, illustration, etc.Object name is pone.0107056.e107.jpg with only one common neighbor. This is in agreement with our intuition very well. Lastly, different nodes may provide different mutual information to reduce the uncertainty of connections. The extent to which node An external file that holds a picture, illustration, etc.Object name is pone.0107056.e108.jpg (An external file that holds a picture, illustration, etc.Object name is pone.0107056.e109.jpg) contributes to the reduction of link uncertainty, for example, is greater than that of node An external file that holds a picture, illustration, etc.Object name is pone.0107056.e110.jpg (An external file that holds a picture, illustration, etc.Object name is pone.0107056.e111.jpg).\"\n",
    "otro = 'Mutual information (MI) is a powerful method for detecting relationships between data sets. There are accurate methods for estimating MI that avoid problems with “binning” when both data sets are discrete or when both data sets are continuous. We present an accurate, non-binning MI estimator for the case of one discrete data set and one continuous data set. This case applies when measuring, for example, the relationship between base sequence and gene expression level, or the effect of a cancer drug on patient survival time. We also show how our method can be adapted to calculate the Jensen–Shannon divergence of two or more data sets.Go to:IntroductionMutual information (MI) [1] is in several ways a perfect statistic for measuring the degree of relatedness between data sets. First, MI will detect any sort of relationship between data sets whatsoever, whether it involves the mean values or the variances or higher moments. Second, MI has a straightforward interpretation as the amount of shared information between data sets (measured in, for example, bits); other statistics such as rank-ordering are harder to interpret. Since MI is grounded in information theory it has an established base of theoretical tools. Finally, MI is insensitive to the size of the data sets. Whereas a ‘p-value’ test for strict independence can be pushed arbitrarily low by taking a large data set if the variables are even slightly related, MI will simply converge with tight error bounds to a measure of their relatedness.The MI between two data sets An external file that holds a picture, illustration, etc.Object name is pone.0087357.e001.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0087357.e002.jpg can be estimated from the statistics of the An external file that holds a picture, illustration, etc.Object name is pone.0087357.e003.jpg pairs between the two data sets. (Although MI is straightforward to calculate if the underlying probability distribution is known, that is not usually the case: our knowledge of the distribution generally comes from the sampled data itself, so MI must be estimated from the statistics of our data set.) For example, if we were to compare the day of week (An external file that holds a picture, illustration, etc.Object name is pone.0087357.e004.jpg) with the time of breakfast (An external file that holds a picture, illustration, etc.Object name is pone.0087357.e005.jpg) we might find that when An external file that holds a picture, illustration, etc.Object name is pone.0087357.e006.jpg is a weekday the corresponding An external file that holds a picture, illustration, etc.Object name is pone.0087357.e007.jpg is early in the morning, and when An external file that holds a picture, illustration, etc.Object name is pone.0087357.e008.jpg is Sunday or (especially) Saturday the corresponding An external file that holds a picture, illustration, etc.Object name is pone.0087357.e009.jpg is somewhat later. MI quantifies the strength of this effect. Importantly, the procedure for estimating MI depends on whether An external file that holds a picture, illustration, etc.Object name is pone.0087357.e010.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0087357.e011.jpg take discrete values (e.g. a day of week, a nucleobase, a phenotypic category, etc.), or are real-valued continuous variables (a time of day, a gene expression level, a patient’s survival time, etc.). If An external file that holds a picture, illustration, etc.Object name is pone.0087357.e012.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0087357.e013.jpg are both discrete, then we can estimate the true frequencies of all combinations of An external file that holds a picture, illustration, etc.Object name is pone.0087357.e014.jpg pairs by counting the number of times each pair occurs in the data, and straightforwardly use these frequencies to estimate MI. Real-valued data sets are more difficult to deal with, since they are by definition sparsely sampled: most real numbers will not be found in a data set of any size. The common workaround is to lump the continuous variables into discrete ‘bins’ and then apply a discrete MI estimator, but good sampling requires large bins which destroys resolution. An improved continuous-continuous MI estimator described in Ref. [2] circumvents this tradeoff by using statistics of the spacings between data points and their nearest neighbors. Crucially, their method only works when both variables are real-valued, as the nearest neighbor of a discrete variable is not well-defined.This paper describes a method for estimating the MI between a discrete data set and a continuous (scalar or vector) data set, using a similar approach to that of Ref. [2]. This is an important statistic simply because so many scientific activities involve a search for significant relationships between discrete and continuous variables. For example, one might use MI to quantify the extent to which nationality (a discrete variable) determines income (continuous); to identify DNA bases (ACGT, discrete) that affect a given gene’s expression level (continuous); or to find drugs (given or not: a discrete parameter) that alter cell division rates (continuous data). In the University of Washington Nanopore Physics lab we use this estimator to determine where a given DNA base must sit within the sequencing pore in order to affect the current passing through it, and to quantify the relative influence of different base positions on the current. As we will demonstrate, our nearest-neighbors method estimates MI much more reliably than does the present alternative method of ‘binning’ the data.MI between a discrete and a continuous variable is equivalent to a weighted form of the Jensen-Shannon (JS) divergence [3] which is used as a measure of the dissimilarity between two or more continuous probability distributions. We can therefore apply our method to estimate the weighted JS divergence, by storing samples from each distribution to be compared in the continuous data set An external file that holds a picture, illustration, etc.Object name is pone.0087357.e015.jpg, and using the discrete data set An external file that holds a picture, illustration, etc.Object name is pone.0087357.e016.jpg to identify which distribution each sample was drawn from. To use our method to estimate the unweighted JS divergence, we would either draw equal numbers of samples from each distribution, or else modify our method somewhat as explained in the Analysis section.Go to:MethodsThis section explains how to apply our nearest-neighbor method for estimating MI; the derivation is left to the Analysis section. We will also describe the binning method that we compare with our estimator.The input to a MI estimator is a list of An external file that holds a picture, illustration, etc.Object name is pone.0087357.e017.jpg data points, whose underlying probability distribution An external file that holds a picture, illustration, etc.Object name is pone.0087357.e018.jpg we can only guess at by looking at how the data points are clustered. Both An external file that holds a picture, illustration, etc.Object name is pone.0087357.e019.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0087357.e020.jpg may be either scalars or vectors. Figure 1A illustrates a simple distribution between a discrete parameter An external file that holds a picture, illustration, etc.Object name is pone.0087357.e021.jpg that can take one of three values denoted by color, and a single scalar real-valued variable An external file that holds a picture, illustration, etc.Object name is pone.0087357.e022.jpg depicted along a y-axis. In this example we see that the different values of An external file that holds a picture, illustration, etc.Object name is pone.0087357.e023.jpg bias the sampling towards different values of An external file that holds a picture, illustration, etc.Object name is pone.0087357.e024.jpg: for example An external file that holds a picture, illustration, etc.Object name is pone.0087357.e025.jpg is generally lower when An external file that holds a picture, illustration, etc.Object name is pone.0087357.e026.jpg is green or red than when An external file that holds a picture, illustration, etc.Object name is pone.0087357.e027.jpg is blue. Therefore there is a relation between An external file that holds a picture, illustration, etc.Object name is pone.0087357.e028.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0087357.e029.jpg, implying that MI is some positive number. The challenge is to estimate MI using only the sampled points that are known to the experimenter (Figure 1B).An external file that holds a picture, illustration, etc.Object name is pone.0087357.g001.jpgFigure 1Procedures for estimating MI.(A) An example joint probability density An external file that holds a picture, illustration, etc.Object name is pone.0087357.e030.jpg where An external file that holds a picture, illustration, etc.Object name is pone.0087357.e031.jpg is a real-valued scalar and An external file that holds a picture, illustration, etc.Object name is pone.0087357.e032.jpg can take one of three values, indicated red, blue and green. For each value of An external file that holds a picture, illustration, etc.Object name is pone.0087357.e033.jpg the probability density in An external file that holds a picture, illustration, etc.Object name is pone.0087357.e034.jpg is shown as plot of that color, whose area is proportional to An external file that holds a picture, illustration, etc.Object name is pone.0087357.e035.jpg. (B) A set of An external file that holds a picture, illustration, etc.Object name is pone.0087357.e036.jpg data pairs sampled from this distribution, where An external file that holds a picture, illustration, etc.Object name is pone.0087357.e037.jpg is represented by the color of each point and An external file that holds a picture, illustration, etc.Object name is pone.0087357.e038.jpg by its position on the An external file that holds a picture, illustration, etc.Object name is pone.0087357.e039.jpg-axis. (C) The computation of An external file that holds a picture, illustration, etc.Object name is pone.0087357.e040.jpg in our nearest-neighbor method. Data point An external file that holds a picture, illustration, etc.Object name is pone.0087357.e041.jpg is the red dot indicated by a vertical arrow. The full data set is on the upper line, and the subset of all red data points is on the lower line. We find that the data point which is the 3rd-closest neighbor to An external file that holds a picture, illustration, etc.Object name is pone.0087357.e042.jpg on the bottom line is the 6th-closest neighbor on the top line. Dashed lines show the distance An external file that holds a picture, illustration, etc.Object name is pone.0087357.e043.jpg from point An external file that holds a picture, illustration, etc.Object name is pone.0087357.e044.jpg out to the 3rd neighbor. An external file that holds a picture, illustration, etc.Object name is pone.0087357.e045.jpg, An external file that holds a picture, illustration, etc.Object name is pone.0087357.e046.jpg, and for this point An external file that holds a picture, illustration, etc.Object name is pone.0087357.e047.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0087357.e048.jpg. (D) A binning of the data into equal bins containing An external file that holds a picture, illustration, etc.Object name is pone.0087357.e049.jpg data points. MI can be estimated from the numbers of points of each color in each bin.Nearest Neighbor MethodFor each data point An external file that holds a picture, illustration, etc.Object name is pone.0087357.e050.jpg our method computes a number An external file that holds a picture, illustration, etc.Object name is pone.0087357.e051.jpg based on its nearest-neighbors in the continuous variable An external file that holds a picture, illustration, etc.Object name is pone.0087357.e052.jpg, as illustrated for scalar An external file that holds a picture, illustration, etc.Object name is pone.0087357.e053.jpg in Figure 1C. We first find the An external file that holds a picture, illustration, etc.Object name is pone.0087357.e054.jpgth-closest neighbor to point An external file that holds a picture, illustration, etc.Object name is pone.0087357.e055.jpg among those An external file that holds a picture, illustration, etc.Object name is pone.0087357.e056.jpg data points whose value of the discrete variable equals An external file that holds a picture, illustration, etc.Object name is pone.0087357.e057.jpg (Figure 1C, bottom line) using some distance metric of our choice. Define An external file that holds a picture, illustration, etc.Object name is pone.0087357.e058.jpg as the distance to this An external file that holds a picture, illustration, etc.Object name is pone.0087357.e059.jpgth neighbor. We then count the number of neighbors An external file that holds a picture, illustration, etc.Object name is pone.0087357.e060.jpg in the full data set (top line) that lie within distance An external file that holds a picture, illustration, etc.Object name is pone.0087357.e061.jpg to point An external file that holds a picture, illustration, etc.Object name is pone.0087357.e062.jpg (including the An external file that holds a picture, illustration, etc.Object name is pone.0087357.e063.jpgth neighbor itself). Based on An external file that holds a picture, illustration, etc.Object name is pone.0087357.e064.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0087357.e065.jpg we computeequation image\t(1)where An external file that holds a picture, illustration, etc.Object name is pone.0087357.e067.jpg is the digamma function [4]. To estimate the MI from our data set, we average An external file that holds a picture, illustration, etc.Object name is pone.0087357.e068.jpg over all data points.equation image\t(2)In our implementation An external file that holds a picture, illustration, etc.Object name is pone.0087357.e070.jpg is some fixed (low) integer of the user’s choice; larger An external file that holds a picture, illustration, etc.Object name is pone.0087357.e071.jpg-values lead to lower sampling error but higher coarse-graining error.Binning MethodWe also implemented a binning method to compare with our nearest-neighbor method. Binning methods make the data completely discrete by grouping the data points into bins in the continuous variable An external file that holds a picture, illustration, etc.Object name is pone.0087357.e072.jpg, as shown in Figure 1D. Following established practice [2] our estimator constructs bins of different sizes so that each bin has An external file that holds a picture, illustration, etc.Object name is pone.0087357.e073.jpg data points inside it (An external file that holds a picture, illustration, etc.Object name is pone.0087357.e074.jpg is a parameter set by the user). The binned approximation to the MI isequation imageequation image\t(3)The average is taken over all measurements An external file that holds a picture, illustration, etc.Object name is pone.0087357.e077.jpg, not the bins. An external file that holds a picture, illustration, etc.Object name is pone.0087357.e078.jpg is the fraction of all measurements whose discrete variable is An external file that holds a picture, illustration, etc.Object name is pone.0087357.e079.jpg, An external file that holds a picture, illustration, etc.Object name is pone.0087357.e080.jpg is the fraction of measurements whose continuous variable falls into the same bin An external file that holds a picture, illustration, etc.Object name is pone.0087357.e081.jpg as An external file that holds a picture, illustration, etc.Object name is pone.0087357.e082.jpg, and An external file that holds a picture, illustration, etc.Object name is pone.0087357.e083.jpg is the fraction of measurements for which An external file that holds a picture, illustration, etc.Object name is pone.0087357.e084.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0087357.e085.jpg falls into bin An external file that holds a picture, illustration, etc.Object name is pone.0087357.e086.jpg. The second line in Eq. 3 follows from the first because we discretize An external file that holds a picture, illustration, etc.Object name is pone.0087357.e087.jpg and An external file that holds a picture, illustration, etc.Object name is pone.0087357.e088.jpg using the same bins.In the Supporting Information we have included two MATLAB implementations of our method: a general-purpose estimator that works with vector-valued data sets, and a faster implementation for the usual case where both data sets are scalars (simple numbers). The Supporting Information also contains our implementation of a MI estimator using the binning method, as well as the testing script that compares the three estimators and generated the plots for this paper.Go to:ResultsTo test our method, we chose two simple distributions An external file that holds a picture, illustration, etc.Object name is pone.0087357.e089.jpg: a square wave distribution in An external file that holds a picture, illustration, etc.Object name is pone.0087357.e090.jpg for each value in An external file that holds a picture, illustration, etc.Object name is pone.0087357.e091.jpg, and a Gaussian distribution in An external file that holds a picture, illustration, etc.Object name is pone.0087357.e092.jpg for each An external file that holds a picture, illustration, etc.Object name is pone.0087357.e093.jpg (Figure 2A). Because we knew the exact form of the distributions, we were able to calculate MI exactly using its mathematical definition:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [wiki_article, otro]                                                                                                                                                                                                   \n",
    "vect = TfidfVectorizer(min_df=1, stop_words=\"english\")                                                                                                                                                                                                   \n",
    "tfidf = vect.fit_transform(corpus)                                                                                                                                                                                                                       \n",
    "pairwise_similarity = tfidf * tfidf.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.72261659],\n",
       "       [0.72261659, 1.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_similarity.toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
